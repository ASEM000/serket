
<div align="center">
<img width="350px" src="assets/serketLogo.svg"></div>
<h2 align="center">The JAX NN Library.</h2>


![Tests](https://github.com/ASEM000/serket/actions/workflows/tests.yml/badge.svg)
![pyver](https://img.shields.io/badge/python-3.7%203.8%203.9%203.10-red)
![codestyle](https://img.shields.io/badge/codestyle-black-lightgrey)
[![codecov](https://codecov.io/gh/ASEM000/serket/branch/main/graph/badge.svg?token=C6NXOK9EVS)](https://codecov.io/gh/ASEM000/serket)

## ğŸ› ï¸ Installation<a id="Installation"></a>

```python
pip install serket
```


## ğŸ“– Description<a id="Description"></a>
- `serket` aims to be the most intuitive and easy-to-use Neural network library in JAX.
- `serket` is built on top of [`pytreeclass`](https://github.com/ASEM000/pytreeclass)
- `serket` currently implements 
  - `Linear`, `FNN`
  - `Dropout`
  - `Sequential`
  - `Lambda`


## â© Quick Example <a id="QuickExample">

Simple Fully connected neural network.

### Model definition
```python

>>> model = Sequential([
    Linear(1,128),
    Lambda(jax.nn.relu),
    Linear(128,128),
    Lambda(jax.nn.relu),
    Linear(128,1),
])

>>> print(model.tree_diagram())
Sequential
    â””â”€â”€ layers=<class 'list'>
        â”œâ”€â”€ layers_0=Linear
        â”‚   â”œâ”€â”€ weight=f32[1,128]
        â”‚   â”œâ”€â”€ bias=f32[128]
        â”‚   â”œ*â”€ in_features=1
        â”‚   â”œ*â”€ out_features=128
        â”‚   â”œ*â”€ weight_init_func=init(key,shape,dtype)
        â”‚   â””*â”€ bias_init_func=Lambda(key,shape)    
        â”œâ”€â”€ layers_1=Lambda
        â”‚   â””*â”€ func=relu(*args,**kwargs)   
        â”œâ”€â”€ layers_2=Linear
        â”‚   â”œâ”€â”€ weight=f32[128,128]
        â”‚   â”œâ”€â”€ bias=f32[128]
        â”‚   â”œ*â”€ in_features=128
        â”‚   â”œ*â”€ out_features=128
        â”‚   â”œ*â”€ weight_init_func=init(key,shape,dtype)
        â”‚   â””*â”€ bias_init_func=Lambda(key,shape)    
        â”œâ”€â”€ layers_3=Lambda
        â”‚   â””*â”€ func=relu(*args,**kwargs)   
        â””â”€â”€ layers_4=Linear
            â”œâ”€â”€ weight=f32[128,1]
            â”œâ”€â”€ bias=f32[1]
            â”œ*â”€ in_features=128
            â”œ*â”€ out_features=1
            â”œ*â”€ weight_init_func=init(key,shape,dtype)
            â””*â”€ bias_init_func=Lambda(key,shape)
```

```python
>>> print(model.summary())
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Name           â”‚Type       â”‚Param #  â”‚Size   â”‚Config             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚layers/layers_0â”‚list/Linearâ”‚256(0)   â”‚1.00KB â”‚weight=f32[1,128]  â”‚
â”‚               â”‚           â”‚         â”‚(0.00B)â”‚bias=f32[128]      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚layers/layers_1â”‚list/Lambdaâ”‚0(0)     â”‚0.00B  â”‚                   â”‚
â”‚               â”‚           â”‚         â”‚(0.00B)â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚layers/layers_2â”‚list/Linearâ”‚16,512(0)â”‚64.50KBâ”‚weight=f32[128,128]â”‚
â”‚               â”‚           â”‚         â”‚(0.00B)â”‚bias=f32[128]      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚layers/layers_3â”‚list/Lambdaâ”‚0(0)     â”‚0.00B  â”‚                   â”‚
â”‚               â”‚           â”‚         â”‚(0.00B)â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚layers/layers_4â”‚list/Linearâ”‚129(0)   â”‚516.00Bâ”‚weight=f32[128,1]  â”‚
â”‚               â”‚           â”‚         â”‚(0.00B)â”‚bias=f32[1]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total count :	16,897(0)
Dynamic count :	16,897(0)
Frozen count :	0(0)
-------------------------------------------------------------------
Total size :	66.00KB(0.00B)
Dynamic size :	66.00KB(0.00B)
Frozen size :	0.00B(0.00B)
===================================================================

```

### Train
```python
x = jnp.linspace(0,1,100)[:,None]
y = x**3 + jax.random.uniform(jax.random.PRNGKey(0),(100,1))*0.01

@jax.value_and_grad
def loss_func(model,x,y):
    return jnp.mean((model(x)-y)**2)

@jax.jit
def update(model,x,y):
    value,grad = loss_func(model,x,y)
    return value , model - 1e-3*grad

for _ in range(20_000):
    value,model = update(model,x,y)
```

### Filter
- Filter by (1)value, (2)`field` name, (3)`field` type, (4)`field` metadata
- See [here](https://github.com/ASEM000/PyTreeClass#%EF%B8%8F-filtering-with-at-) for more
