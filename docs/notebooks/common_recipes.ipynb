{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ³  Common recipes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces common recipes you might need while using `serket` to train/build models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/ASEM000/serket --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Per-leaf optimization\n",
    "The following recipe, `optax.masked` is used to apply certain optmizers to certain leaves using masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple\n",
      "â”œâ”€â”€ [0]:MaskedState\n",
      "â”‚   â””â”€â”€ .inner_state:tuple\n",
      "â”‚       â””â”€â”€ [0]:ScaleByAdamState\n",
      "â”‚           â”œâ”€â”€ .count=i32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0,0])\n",
      "â”‚           â”œâ”€â”€ .mu:Tree\n",
      "â”‚           â”‚   â””â”€â”€ .a=f32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
      "â”‚           â””â”€â”€ .nu:Tree\n",
      "â”‚               â””â”€â”€ .a=f32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
      "â”œâ”€â”€ [1]:MaskedState\n",
      "â”‚   â””â”€â”€ .inner_state:tuple\n",
      "â”‚       â””â”€â”€ [0]:ScaleByAdamState\n",
      "â”‚           â”œâ”€â”€ .count=i32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0,0])\n",
      "â”‚           â”œâ”€â”€ .mu:Tree\n",
      "â”‚           â”‚   â””â”€â”€ .b=f32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
      "â”‚           â””â”€â”€ .nu:Tree\n",
      "â”‚               â””â”€â”€ .b=f32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
      "â””â”€â”€ [2]:MaskedState\n",
      "    â””â”€â”€ .inner_state:tuple\n",
      "        â””â”€â”€ [0]:ScaleByAdamState\n",
      "            â”œâ”€â”€ .count=i32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0,0])\n",
      "            â”œâ”€â”€ .mu:Tree\n",
      "            â”‚   â””â”€â”€ .c=f32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
      "            â””â”€â”€ .nu:Tree\n",
      "                â””â”€â”€ .c=f32[](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "import serket as sk\n",
    "import jax\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class Tree(sk.TreeClass):\n",
    "    a: float = 1.0\n",
    "    b: float = 2.0\n",
    "    c: float = 3.0\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "\n",
    "false_mask = tree.at[...].set(False)\n",
    "\n",
    "a_mask = false_mask.at[\"a\"].set(True)\n",
    "b_mask = false_mask.at[\"b\"].set(True)\n",
    "c_mask = false_mask.at[\"c\"].set(True)\n",
    "\n",
    "optim = optax.chain(\n",
    "    # update `a` with sgd of learning rate 1\n",
    "    optax.masked(optax.adam(learning_rate=1), a_mask),\n",
    "    # update `b` with sgd of learning rate -1\n",
    "    optax.masked(optax.adam(learning_rate=-1), b_mask),\n",
    "    # update `c` with sgd of learning rate 0\n",
    "    optax.masked(optax.adam(learning_rate=0), c_mask),\n",
    ")\n",
    "optim_state = optim.init(sk.tree_mask(tree))\n",
    "# the optimizer contains 3 sub-optimizers for each field of the tree\n",
    "print(sk.tree_diagram(optim_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Buffers\n",
    "In this example, certain array will be marked as non-trainable using `jax.lax.stop_gradient` and `field`.\n",
    "\n",
    "The standard way to mark an array as a buffer (e.g. non-trainable) is to write something like this:\n",
    "```python\n",
    "class Tree(sk.TreeClass):\n",
    "    def __init__(self, buffer: jax.Array):\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return x + jax.lax.stop_gradient(self.buffer)\n",
    "```\n",
    "However, if you access this buffer from other methods, then it another `jax.lax.stop_gradient` should be used and written inside all the methods:\n",
    "\n",
    "```python\n",
    "class Tree(sk.TreeClass):\n",
    "    def method_1(self, x: jax.Array) -> jax.Array:\n",
    "        return x + jax.lax.stop_gradient(self.buffer)\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "    def method_n(self, x: jax.Array) -> jax.Array:\n",
    "        return x + jax.lax.stop_gradient(self.buffer)\n",
    "```\n",
    "\n",
    "Similarly, if you access `buffer` defined for `Tree` instances, from another context, you need to use `jax.lax.stop_gradient` again:\n",
    "\n",
    "```python\n",
    "tree = Tree(buffer=...)\n",
    "def func(tree: Tree):\n",
    "    buffer = jax.lax.stop_gradient(tree.buffer)\n",
    "    ...    \n",
    "```\n",
    "\n",
    "This becomes cumbersome if this process is repeated multiple times. for this, applying `jax.lax.stop_gradient` on `__getattr__` using `on_getattr` is simpler to use, because you need to define it only once. \n",
    "\n",
    "The following example demonstrate this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "Tree(buffer=[0. 0. 0.])\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def buffer_field(**kwargs):\n",
    "    return sk.field(on_getattr=[jax.lax.stop_gradient], **kwargs)\n",
    "\n",
    "\n",
    "@sk.autoinit  # autoinit construct `__init__` from fields\n",
    "class Tree(sk.TreeClass):\n",
    "    buffer: jax.Array = buffer_field()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.buffer**x\n",
    "\n",
    "\n",
    "tree = Tree(buffer=jnp.array([1.0, 2.0, 3.0]))\n",
    "tree(2.0)  # Array([1., 4., 9.], dtype=float32)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(tree: Tree, x: jax.Array):\n",
    "    return jnp.sum(tree(x))\n",
    "\n",
    "\n",
    "print(f(tree, 1.0))\n",
    "print(jax.grad(f)(tree, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Frozen fields\n",
    "\n",
    "In this example, field value freezing is done on class level using `on_geatattr`, and `on_setattr`. This effectively hide the field value across `jax` transformation.\n",
    "\n",
    "Hiding a field means that this field value does not get traced/updated by `jax` internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "Tree(frozen_a=#1)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "\n",
    "\n",
    "def frozen_field(**kwargs):\n",
    "    return sk.field(on_getattr=[sk.unfreeze], on_setattr=[sk.freeze], **kwargs)\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class Tree(sk.TreeClass):\n",
    "    frozen_a: int = frozen_field()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.frozen_a + x\n",
    "\n",
    "\n",
    "tree = Tree(frozen_a=1)  # 1 is non-jaxtype\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(tree, x):\n",
    "    return tree(x)\n",
    "\n",
    "\n",
    "print(f(tree, 1.0))\n",
    "\n",
    "print(jax.grad(f)(tree, 1.0))\n",
    "\n",
    "# not visible to `jax.tree_util...`\n",
    "print(jax.tree_util.tree_leaves(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unfreeze the frozen values, use `tree_unmask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(jax.tree_util.tree_leaves(sk.tree_unmask(tree)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Parameterization\n",
    "\n",
    "In this example, field value is [parameterized](https://pytorch.org/tutorials/intermediate/parametrizations.html) using `on_getattr`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 1  8  5]\n",
      " [ 2  5 16]]\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def symmetric(array: jax.Array) -> jax.Array:\n",
    "    triangle = jnp.triu(array)  # upper triangle\n",
    "    return triangle + triangle.transpose(-1, -2)\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class Tree(sk.TreeClass):\n",
    "    symmetric_matrix: jax.Array = sk.field(on_getattr=[symmetric])\n",
    "\n",
    "\n",
    "tree = Tree(symmetric_matrix=jnp.arange(9).reshape(3, 3))\n",
    "print(tree.symmetric_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] `numpy` and `TreeClass`.\n",
    "\n",
    "In this recipe, `numpy` functions will operate directly on `TreeClass` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree(a=0, b=(0.0, 103.0), c=[104. 105. 106.])\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@sk.leafwise  # enable math operations on leaves\n",
    "@sk.autoinit  # generate __init__ from type annotations\n",
    "class Tree(sk.TreeClass):\n",
    "    a: int = 1\n",
    "    b: tuple[float] = (2.0, 3.0)\n",
    "    c: jax.Array = jnp.array([4.0, 5.0, 6.0])\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "\n",
    "# make where work with arbitrary pytrees\n",
    "tree_where = sk.bcmap(jnp.where)\n",
    "# for values > 2, add 100, else set to 0\n",
    "print(tree_where(tree > 2, tree + 100, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] Validate and convert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type and number range check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On applying Range(min=1, max=100) for field=`in_dim`:\n",
      "0 not in range [1, 100]\n",
      "On applying IsInstance(klass=<class 'int'>) for field=`in_dim`:\n",
      "1.0 not an instance of <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import serket as sk\n",
    "\n",
    "\n",
    "# you can use any function\n",
    "@sk.autoinit\n",
    "class Range(sk.TreeClass):\n",
    "    min: int | float = -float(\"inf\")\n",
    "    max: int | float = float(\"inf\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not (self.min <= x <= self.max):\n",
    "            raise ValueError(f\"{x} not in range [{self.min}, {self.max}]\")\n",
    "        return x\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class IsInstance(sk.TreeClass):\n",
    "    klass: type | tuple[type, ...]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not isinstance(x, self.klass):\n",
    "            raise TypeError(f\"{x} not an instance of {self.klass}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class Foo(sk.TreeClass):\n",
    "    # allow in_dim to be an integer between [1,100]\n",
    "    in_dim: int = sk.field(on_setattr=[IsInstance(int), Range(1, 100)])\n",
    "\n",
    "\n",
    "tree = Foo(1)\n",
    "# no error\n",
    "\n",
    "try:\n",
    "    tree = Foo(0)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    tree = Foo(1.0)\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array shape and dtype check, then dtype conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
      "Size mismatch, 3 != 1 at dimension 0 \n",
      "\n",
      "On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
      "Dtype mismatch, array_dtype=dtype('float16') != self.dtype=<class 'jax.numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "from typing import Any\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class ArrayValidator(sk.TreeClass):\n",
    "    def __init__(self, shape, dtype):\n",
    "        \"\"\"Validate shape and dtype of input array.\n",
    "\n",
    "        Args:\n",
    "            shape: Expected shape of array. available values are int, None, ...\n",
    "                use int for fixed size, None for any size, and ... for any number\n",
    "                of dimensions. for example (..., 1) allows any number of dimensions\n",
    "                with the last dimension being 1. (1, ..., 1) allows any number of\n",
    "                dimensions with the first and last dimensions being 1.\n",
    "            dtype: Expected dtype of array.\n",
    "\n",
    "        Example:\n",
    "            >>> x = jnp.ones((5, 5))\n",
    "            >>> # any number of dimensions with last dim=5\n",
    "            >>> shape = (..., 5)\n",
    "            >>> dtype = jnp.float32\n",
    "            >>> validator = ArrayValidator(shape, dtype)\n",
    "            >>> validator(x)  # no error\n",
    "\n",
    "            >>> # must be 2 dimensions with first dim unconstrained and last dim=5\n",
    "            >>> shape = (None, 5)\n",
    "            >>> validator = ArrayValidator(shape, dtype)\n",
    "            >>> validator(x)  # no error\n",
    "        \"\"\"\n",
    "\n",
    "        if shape.count(...) > 1:\n",
    "            raise ValueError(\"Only one ellipsis allowed\")\n",
    "\n",
    "        for si in shape:\n",
    "            if not isinstance(si, (int, type(...), type(None))):\n",
    "                raise TypeError(f\"Expected int or ..., got {si}\")\n",
    "\n",
    "        self.shape = shape\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not (hasattr(x, \"shape\") and hasattr(x, \"dtype\")):\n",
    "            raise TypeError(f\"Expected array with shape {self.shape}, got {x}\")\n",
    "\n",
    "        shape = list(self.shape)\n",
    "        array_shape = list(x.shape)\n",
    "        array_dtype = x.dtype\n",
    "\n",
    "        if self.shape and array_dtype != self.dtype:\n",
    "            raise TypeError(f\"Dtype mismatch, {array_dtype=} != {self.dtype=}\")\n",
    "\n",
    "        if ... in shape:\n",
    "            index = shape.index(...)\n",
    "            shape = (\n",
    "                shape[:index]\n",
    "                + [None] * (len(array_shape) - len(shape) + 1)\n",
    "                + shape[index + 1 :]\n",
    "            )\n",
    "\n",
    "        if len(shape) != len(array_shape):\n",
    "            raise ValueError(f\"{len(shape)=} != {len(array_shape)=}\")\n",
    "\n",
    "        for i, (li, ri) in enumerate(zip(shape, array_shape)):\n",
    "            if li is None:\n",
    "                continue\n",
    "            if li != ri:\n",
    "                raise ValueError(f\"Size mismatch, {li} != {ri} at dimension {i}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "# any number of dimensions with firt dim=3 and last dim=6\n",
    "shape = (3, ..., 6)\n",
    "# dtype must be float32\n",
    "dtype = jnp.float32\n",
    "\n",
    "validator = ArrayValidator(shape=shape, dtype=dtype)\n",
    "\n",
    "# convert to half precision from float32\n",
    "converter = lambda x: x.astype(jnp.float16)\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class Tree(sk.TreeClass):\n",
    "    array: jax.Array = sk.field(on_setattr=[validator, converter])\n",
    "\n",
    "\n",
    "x = jnp.ones([3, 1, 2, 6])\n",
    "tree = Tree(array=x)\n",
    "\n",
    "\n",
    "try:\n",
    "    y = jnp.ones([1, 1, 2, 3])\n",
    "    tree = Tree(array=y)\n",
    "except ValueError as e:\n",
    "    print(e, \"\\n\")\n",
    "    # On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
    "    # Dtype mismatch, array_dtype=dtype('float16') != self.dtype=<class 'jax.numpy.float32'>\n",
    "\n",
    "try:\n",
    "    z = x.astype(jnp.float16)\n",
    "    tree = Tree(array=z)\n",
    "except TypeError as e:\n",
    "    print(e)\n",
    "    # On applying ArrayValidator(shape=(3, Ellipsis, 6), dtype=<class 'jax.numpy.float32'>) for field=`array`:\n",
    "    # Size mismatch, 3 != 1 at dimension 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [7] Lazy layers.\n",
    "In this example, a `Linear` layer with a weight parameter based on the shape of the input will be created. Since this requires parameter creation (i.e., `weight`) after instance initialization, we will use `.at` to create a new instance with the added parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer before param is set:\tLazyLinear(out_features=1)\n",
      "Layer after param is set:\tLazyLinear(out_features=1, weight=[[1.]], bias=[0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import serket as sk\n",
    "from typing import Any\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class LazyLinear(sk.TreeClass):\n",
    "    out_features: int\n",
    "\n",
    "    def param(self, name: str, value: Any):\n",
    "        # return the value if it exists, otherwise set it and return it\n",
    "        if name not in vars(self):\n",
    "            setattr(self, name, value)\n",
    "        return vars(self)[name]\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, key: jax.Array = jr.PRNGKey(0)):\n",
    "        weight = self.param(\"weight\", jnp.ones((x.shape[-1], self.out_features)))\n",
    "        bias = self.param(\"bias\", jnp.zeros((self.out_features,)))\n",
    "        return x @ weight + bias\n",
    "\n",
    "\n",
    "x = jnp.ones([10, 1])\n",
    "\n",
    "lazy_linear = LazyLinear(out_features=1)\n",
    "\n",
    "lazy_linear\n",
    "print(f\"Layer before param is set:\\t{lazy_linear}\")\n",
    "\n",
    "\n",
    "# first call will set the parameters\n",
    "_, linear = lazy_linear.at[\"__call__\"](x, key=jr.PRNGKey(0))\n",
    "\n",
    "print(f\"Layer after param is set:\\t{linear}\")\n",
    "# subsequent calls will use the same parameters and not set them again\n",
    "linear(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] Intermediates handling.\n",
    "\n",
    "This example shows how to capture specific intermediate values within each function call in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate values:\t\n",
      " (Array([[0. ],\n",
      "       [0.5],\n",
      "       [1. ],\n",
      "       [1.5],\n",
      "       [2. ]], dtype=float32), Array([[-0.09999937],\n",
      "       [ 0.40000063],\n",
      "       [ 0.90000063],\n",
      "       [ 1.4000006 ],\n",
      "       [ 1.9000006 ]], dtype=float32))\n",
      "\n",
      "Final tree:\t\n",
      " Tree(a=0.801189)\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import serket as sk\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class Tree(sk.TreeClass):\n",
    "    a: float = 1.0\n",
    "\n",
    "    def __call__(self, x: jax.Array, intermediate: tuple[Any, ...]):\n",
    "        x = x + self.a\n",
    "        # store intermediate variables\n",
    "        return x, intermediate + (x,)\n",
    "\n",
    "\n",
    "def loss_func(tree: Tree, x: jax.Array, y: jax.Array, intermediate: tuple[Any, ...]):\n",
    "    ypred, intermediate = tree(x, intermediate)\n",
    "    loss = jnp.mean((ypred - y) ** 2)\n",
    "    return loss, intermediate\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    tree: Tree,\n",
    "    optim_state: optax.OptState,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    intermediate: tuple[Any, ...],\n",
    "):\n",
    "    grads, intermediate = jax.grad(loss_func, has_aux=True)(tree, x, y, intermediate)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    tree = optax.apply_updates(tree, updates)\n",
    "    return tree, optim_state, intermediate\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "optim = optax.adam(1e-1)\n",
    "optim_state = optim.init(tree)\n",
    "\n",
    "x = jnp.linspace(-1, 1, 5)[:, None]\n",
    "y = x**2\n",
    "\n",
    "intermediate = ()\n",
    "\n",
    "for i in range(2):\n",
    "    tree, optim_state, intermediate = train_step(tree, optim_state, x, y, intermediate)\n",
    "\n",
    "\n",
    "print(\"Intermediate values:\\t\\n\", intermediate)\n",
    "print(\"\\nFinal tree:\\t\\n\", tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [9] Layers from configurations.\n",
    "The next example shows how to use `serket.bcmap` to loop over a configuration dictionary that defines creation of simple linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(\n",
       "   weight=f32[1,1](Î¼=0.31, Ïƒ=0.00, âˆˆ[0.31,0.31]), \n",
       "   bias=f32[1](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
       " ),\n",
       " Linear(\n",
       "   weight=f32[2,1](Î¼=-1.27, Ïƒ=0.33, âˆˆ[-1.59,-0.94]), \n",
       "   bias=f32[1](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
       " ),\n",
       " Linear(\n",
       "   weight=f32[3,1](Î¼=0.24, Ïƒ=0.53, âˆˆ[-0.48,0.77]), \n",
       "   bias=f32[1](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
       " ),\n",
       " Linear(\n",
       "   weight=f32[4,1](Î¼=-0.28, Ïƒ=0.21, âˆˆ[-0.64,-0.08]), \n",
       "   bias=f32[1](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
       " )]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "\n",
    "\n",
    "class Linear(sk.TreeClass):\n",
    "    def __init__(self, in_dim: int, out_dim: int, *, key: jax.Array):\n",
    "        self.weight = jax.random.normal(key, (in_dim, out_dim))\n",
    "        self.bias = jnp.zeros((out_dim,))\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "\n",
    "config = {\n",
    "    # each layer gets a different input dimension\n",
    "    \"in_dim\": [1, 2, 3, 4],\n",
    "    # out_dim is broadcasted to all layers\n",
    "    \"out_dim\": 1,\n",
    "    # each layer gets a different key\n",
    "    \"key\": list(jax.random.split(jax.random.PRNGKey(0), 4)),\n",
    "}\n",
    "\n",
    "\n",
    "# `bcmap` transforms a function that takes a single input into a function that\n",
    "# arbitrary pytree inputs. in case of a single input, the input is broadcasted\n",
    "# to match the tree structure of the first argument\n",
    "# (in our example is a list of 4 inputs)\n",
    "\n",
    "\n",
    "@sk.bcmap\n",
    "def build_layer(in_dim, out_dim, *, key: jax.Array):\n",
    "    return Linear(in_dim, out_dim, key=key)\n",
    "\n",
    "\n",
    "build_layer(config[\"in_dim\"], config[\"out_dim\"], key=config[\"key\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10] Ensembling\n",
    "In this example, simple `Linear` layers are grouped by their weight on the first axis using `jax.vmap`. This is useful if the different instances of the model are desired to run in a vectorized fashion (model ensemble).\n",
    "\n",
    "For more check [here](http://matpalm.com/blog/ensemble_nets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single input ensemble shape:\t(4, 10, 1)\n",
      "Multi input ensemble shape:\t(4, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import serket as sk\n",
    "import functools as ft\n",
    "from typing import Generic, TypeVar\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "class Batched(Generic[T]):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Linear(sk.TreeClass):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        *,\n",
    "        key: jax.Array,\n",
    "        name: str,\n",
    "    ):\n",
    "        self.weight = jr.normal(key, (in_dim, out_dim))\n",
    "        self.bias = jnp.zeros((out_dim,))\n",
    "        self.name = name  # non-jax type for `tree_mask`/`tree_unmask` demonstration\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "\n",
    "class FNN(sk.TreeClass):\n",
    "    def __init__(self, key: jax.Array):\n",
    "        k1, k2, k3 = jr.split(key, 3)\n",
    "        self.l1 = Linear(1, 10, key=k1, name=\"l1\")\n",
    "        self.l2 = Linear(10, 10, key=k2, name=\"l2\")\n",
    "        self.l3 = Linear(10, 1, key=k3, name=\"l3\")\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        x = self.l1(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def build_ensemble(keys: jax.Array) -> Batched[FNN]:\n",
    "    @jax.vmap\n",
    "    def build_liner(key: jax.Array):\n",
    "        # `jax.vmap` require jax-type return\n",
    "        # so use `tree_mask` on return\n",
    "        return sk.tree_mask(FNN(key=key))\n",
    "\n",
    "    return sk.tree_unmask(build_liner(keys))\n",
    "\n",
    "\n",
    "def run_single_input_ensemble(fnns: Batched[FNN], x: jax.Array):\n",
    "    def run_linear(fnn: FNN):\n",
    "        # `jax.vmap` require jax-type return\n",
    "        # so use `tree_mask` on return\n",
    "        return sk.tree_mask(fnn(x))\n",
    "\n",
    "    return jax.vmap(run_linear)(sk.tree_mask(fnns))\n",
    "\n",
    "\n",
    "def run_multi_input_ensemble(fnns: Batched[FNN], x: Batched[jax.Array]):\n",
    "    def run_linear(fnn: FNN, x: jax.Array):\n",
    "        # `jax.vmap` require jax-type return\n",
    "        # so use `tree_mask` on return\n",
    "        return sk.tree_mask(fnn(x))\n",
    "\n",
    "    return jax.vmap(run_linear)(sk.tree_mask(fnns), x)\n",
    "\n",
    "\n",
    "num_layers = 4\n",
    "keys = jr.split(jr.PRNGKey(0), num_layers)\n",
    "\n",
    "# single input ensemble\n",
    "# e.g. each model in the ensemble gets the same input\n",
    "x = jnp.ones([10, 1])\n",
    "fnns = build_ensemble(keys=keys)\n",
    "y = run_single_input_ensemble(fnns, x)\n",
    "print(f\"Single input ensemble shape:\\t{y.shape}\")\n",
    "\n",
    "# multi input ensemble\n",
    "# e.g. each model in the ensemble gets a different input\n",
    "xs = jnp.stack([x, x * 2, x * 3, x * 4])\n",
    "fnns = build_ensemble(keys=keys)\n",
    "ys = run_multi_input_ensemble(fnns, xs)\n",
    "print(f\"Multi input ensemble shape:\\t{ys.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [11] Data pipelines\n",
    "\n",
    "In this example, `AtIndexer` is used in similar fashion to [PyFunctional](https://github.com/EntilZha/PyFunctional) to work on general data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from serket import AtIndexer\n",
    "import jax\n",
    "\n",
    "\n",
    "class Transaction:\n",
    "    def __init__(self, reason, amount):\n",
    "        self.reason = reason\n",
    "        self.amount = amount\n",
    "\n",
    "\n",
    "# this example copied from  https://github.com/EntilZha/PyFunctional\n",
    "transactions = [\n",
    "    Transaction(\"github\", 7),\n",
    "    Transaction(\"food\", 10),\n",
    "    Transaction(\"coffee\", 5),\n",
    "    Transaction(\"digitalocean\", 5),\n",
    "    Transaction(\"food\", 5),\n",
    "    Transaction(\"riotgames\", 25),\n",
    "    Transaction(\"food\", 10),\n",
    "    Transaction(\"amazon\", 200),\n",
    "    Transaction(\"paycheck\", -1000),\n",
    "]\n",
    "\n",
    "indexer = AtIndexer(transactions)\n",
    "where = jax.tree_map(lambda x: x.reason == \"food\", transactions)\n",
    "food_cost = indexer[where].reduce(lambda x, y: x + y.amount, initializer=0)\n",
    "food_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [12] Regularization\n",
    "\n",
    "The following code showcase how to use `at` functionality to select some leaves of a model based on boolean mask or/and name condition to apply some weight regualrization on them. For example using `.at[...]` functionality the following can be achieved concisely:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean-based mask\n",
    "\n",
    "The entries of the arrays or leaves are selected based on a tree of the same structure but with boolean (`True`/`False`) leave. The `True` leaf points to place where the operation can be done, while `False` leaf is indicating that this leaf should not be touched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8333335\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "class Net(sk.TreeClass):\n",
    "    def __init__(self):\n",
    "        self.weight = jnp.array([-1, -2, -3, 1, 2, 3])\n",
    "        self.bias = jnp.array([-1, 1])\n",
    "\n",
    "\n",
    "def negative_entries_l2_loss(net: Net):\n",
    "    return (\n",
    "        # select all positive array entries\n",
    "        net.at[jax.tree_map(lambda x: x > 0, net)]\n",
    "        # set them to zero to exclude their loss\n",
    "        .set(0)\n",
    "        # select all leaves\n",
    "        .at[...]\n",
    "        # finally reduce with l2 loss\n",
    "        .reduce(lambda x, y: x + jnp.mean(y**2), initializer=0)\n",
    "    )\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(negative_entries_l2_loss(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name-based mask\n",
    "\n",
    "In this step, the mask is based on the path of the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(\n",
      "  in_features=(1), \n",
      "  out_features=1, \n",
      "  weight_init=glorot_uniform, \n",
      "  bias_init=zeros, \n",
      "  weight=f32[1,1](Î¼=0.20, Ïƒ=0.00, âˆˆ[0.20,0.20]), \n",
      "  bias=f32[1](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# note that `weight` is a leaf node in this layer\n",
    "# the `weight` leaf will be selected later in the next example.\n",
    "print(repr(sk.nn.Linear(1, 1, key=jax.random.PRNGKey(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.83809\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Net(sk.TreeClass):\n",
    "    def __init__(self, key: jax.Array) -> None:\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.linear1 = sk.nn.Linear(in_features=1, out_features=20, key=k1)\n",
    "        self.linear2 = sk.nn.Linear(in_features=20, out_features=20, key=k2)\n",
    "        self.linear3 = sk.nn.Linear(in_features=20, out_features=20, key=k3)\n",
    "        self.linear4 = sk.nn.Linear(in_features=20, out_features=1, key=k4)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = jax.nn.tanh(self.linear1(x))\n",
    "        x = jax.nn.tanh(self.linear2(x))\n",
    "        x = jax.nn.tanh(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def linear_12_weight_l1_loss(net: Net):\n",
    "    return (\n",
    "        # select desired branches (linear1, linear2 in this example)\n",
    "        # and the desired leaves (weight)\n",
    "        net.at[\"linear1\", \"linear2\"][\"weight\"]\n",
    "        # alternatively, regex can be used to do the same functiontality\n",
    "        # >>> import re\n",
    "        # >>> net.at[re.compile(\"linear[12]\")][\"weight\"]\n",
    "        # finally apply l1 loss\n",
    "        .reduce(lambda x, y: x + jnp.sum(jnp.abs(y)), initializer=0)\n",
    "    )\n",
    "\n",
    "\n",
    "net = Net(key=jax.random.PRNGKey(0))\n",
    "print(linear_12_weight_l1_loss(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe can then be included inside the loss function, for example\n",
    "\n",
    "``` python\n",
    "\n",
    "def loss_fnc(net, x, y):\n",
    "    l1_loss = linear_12_weight_l1_loss(net)\n",
    "    loss += l1_loss\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [13] Sharing/Tie Weights\n",
    "\n",
    "In this example a simple `AutoEncoder` with shared `weight` between the encode/decoder is demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TiedAutoEncoder(\n",
       "  encoder=Linear(\n",
       "    in_features=(#1), \n",
       "    out_features=#10, \n",
       "    weight_init=#glorot_uniform, \n",
       "    bias_init=#zeros, \n",
       "    weight=f32[1,10](Î¼=-0.78, Ïƒ=1.11, âˆˆ[-2.58,0.00]), \n",
       "    bias=f32[10](Î¼=-0.39, Ïƒ=0.55, âˆˆ[-1.29,0.00])\n",
       "  ), \n",
       "  decoder=Linear(\n",
       "    in_features=(#10), \n",
       "    out_features=#1, \n",
       "    weight_init=#glorot_uniform, \n",
       "    bias_init=#zeros, \n",
       "    weight=None, \n",
       "    bias=f32[1](Î¼=-2.40, Ïƒ=0.00, âˆˆ[-2.40,-2.40])\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "\n",
    "class TiedAutoEncoder(sk.TreeClass):\n",
    "    def __init__(self, *, key: jax.Array):\n",
    "        k1, k2 = jr.split(key)\n",
    "        self.encoder = sk.nn.Linear(1, 10, key=k1)\n",
    "        # set the unused weight of decoder to `None` to avoid memory usage\n",
    "        self.decoder = sk.nn.Linear(10, 1, key=k2).at[\"weight\"].set(None)\n",
    "\n",
    "    def _call(self, x):\n",
    "        # share/tie weights of encoder and decoder\n",
    "        # however this operation mutates the state\n",
    "        # so this method will only work with .at\n",
    "        # otherwise will throw `AttributeError`\n",
    "        self.decoder.weight = self.encoder.weight.T\n",
    "        out = self.decoder(jax.nn.relu(self.encoder(x)))\n",
    "        return out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # make the mutating method `_call` work with .at\n",
    "        # since .at returns a tuple of the method value and a new instance\n",
    "        # of the class that has the mutated state (i.e. does not mutate in place)\n",
    "        # then we can define __call__ to return only the result of the method\n",
    "        # and ignore the new instance of the class\n",
    "        out, _ = self.at[\"_call\"](x)\n",
    "        return out\n",
    "\n",
    "\n",
    "tree = sk.tree_mask(TiedAutoEncoder(key=jr.PRNGKey(0)))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@jax.grad\n",
    "def loss_func(net, x, y):\n",
    "    net = sk.tree_unmask(net)\n",
    "    return jnp.mean((jax.vmap(net)(x) - y) ** 2)\n",
    "\n",
    "\n",
    "tree = sk.tree_mask(tree)\n",
    "x = jnp.ones([10, 1]) + 0.0\n",
    "y = jnp.ones([10, 1]) * 2.0\n",
    "grads: TiedAutoEncoder = loss_func(tree, x, y)\n",
    "\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [14] Masked transformation\n",
    "\n",
    "As an alternative to using `sk.tree_unmask` on pytrees before calling the function -as seen throughout training examples and recipes- , another approach is to wrap a certain transformation - not pytrees - (e.g. `jit`) to be make the masking/unmasking automatic; however this apporach will incur more overhead than applying `sk.tree_unmask` before the function call.\n",
    "\n",
    "The following example demonstrate how to wrap `jit`, and `vmap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serket as sk\n",
    "import functools as ft\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def automask(jax_transform):\n",
    "    \"\"\"Enable jax transformations to accept non-jax types.\"\"\"\n",
    "\n",
    "    def out_transform(func, **transformation_kwargs):\n",
    "        @ft.partial(jax_transform, **transformation_kwargs)\n",
    "        def jax_boundary(*args, **kwargs):\n",
    "            # unmask the inputs before pasing to the actual function\n",
    "            args, kwargs = sk.tree_unmask((args, kwargs))\n",
    "            # outputs should return jax types\n",
    "            return sk.tree_mask(func(*args, **kwargs))\n",
    "\n",
    "        @ft.wraps(func)\n",
    "        def outer_wrapper(*args, **kwargs):\n",
    "            # mask the inputs before the `jax` boundary\n",
    "            args, kwargs = sk.tree_mask((args, kwargs))\n",
    "            # apply the jax transformation\n",
    "            output = jax_boundary(*args, **kwargs)\n",
    "            # unmask the outputs before returning\n",
    "            return sk.tree_unmask(output)\n",
    "\n",
    "        return outer_wrapper\n",
    "\n",
    "    return out_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`automask` with `jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`jit error`: Argument 'layer' of type <class 'str'> is not a valid JAX type\n",
      "Using automask:\n",
      "forward_jit(params, x)=Array([[4.999546, 4.999546, 4.999546, 4.999546, 4.999546],\n",
      "       [4.999546, 4.999546, 4.999546, 4.999546, 4.999546],\n",
      "       [4.999546, 4.999546, 4.999546, 4.999546, 4.999546],\n",
      "       [4.999546, 4.999546, 4.999546, 4.999546, 4.999546],\n",
      "       [4.999546, 4.999546, 4.999546, 4.999546, 4.999546]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x, y = jnp.ones([5, 5]), jnp.ones([5, 5])\n",
    "\n",
    "params = dict(w1=jnp.ones([5, 5]), w2=jnp.ones([5, 5]), name=\"layer\")\n",
    "\n",
    "\n",
    "def forward(params: dict[str, Any], x: jax.Array) -> jax.Array:\n",
    "    return jnp.tanh(x @ params[\"w1\"]) @ params[\"w2\"]\n",
    "\n",
    "\n",
    "try:\n",
    "    forward_jit = jax.jit(forward)\n",
    "    print(forward_jit(params, x))\n",
    "except TypeError as e:\n",
    "    print(\"`jit error`:\", e)\n",
    "    # now with `automask` the function can accept non-jax types (e.g. string)\n",
    "    forward_jit = automask(jax.jit)(forward)\n",
    "    print(\"Using automask:\")\n",
    "    print(f\"{forward_jit(params, x)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`automask` with `vmap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`vmap error`: Output from batched function 'layer' with type <class 'str'> is not a valid JAX type\n",
      "Using automask:\n",
      "{\n",
      "  name:layer, \n",
      "  w1:f32[4,5,5](Î¼=0.50, Ïƒ=0.28, âˆˆ[0.02,1.00]), \n",
      "  w2:f32[4,5,5](Î¼=0.46, Ïƒ=0.27, âˆˆ[0.01,0.99])\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def make_params(key: jax.Array):\n",
    "    k1, k2 = jax.random.split(key.astype(jnp.uint32))\n",
    "    return dict(w1=jr.uniform(k1, (5, 5)), w2=jr.uniform(k2, (5, 5)), name=\"layer\")\n",
    "\n",
    "\n",
    "keys = jr.split(jr.PRNGKey(0), 4).astype(jnp.float32)\n",
    "\n",
    "try:\n",
    "    params = jax.vmap(make_params)(keys)\n",
    "    print(params)\n",
    "except TypeError as e:\n",
    "    print(\"`vmap error`:\", e)\n",
    "    # now with `automask` the function can accept non-jax types (e.g. string)\n",
    "    params = automask(jax.vmap)(make_params)(keys)\n",
    "    print(\"Using automask:\")\n",
    "    print(sk.tree_repr(params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
