{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—‚ï¸  Misc recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ASEM000/serket --quiet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section introduces some miscellaneous recipes that are not covered in the previous sections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Lazy layers.\n",
    "In this example, a `Linear` layer with a weight parameter based on the shape of the input will be created. Since this requires parameter creation (i.e., `weight`) after instance initialization, we will use `value_and_tree` to create a new instance with the added parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer before param is set:\tLazyLinear(out_features=1)\n",
      "Layer after param is set:\tLazyLinear(out_features=1, weight=[[1.]], bias=[0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import serket as sk\n",
    "from typing import Any\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "\n",
    "class LazyLinear(sk.TreeClass):\n",
    "    def __init__(self, out_features: int):\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def param(self, name: str, value: Any):\n",
    "        # return the value if it exists, otherwise set it and return it\n",
    "        if name not in vars(self):\n",
    "            setattr(self, name, value)\n",
    "        return vars(self)[name]\n",
    "\n",
    "    def __call__(self, input: jax.Array) -> jax.Array:\n",
    "        weight = self.param(\"weight\", jnp.ones((self.out_features, input.shape[-1])))\n",
    "        bias = self.param(\"bias\", jnp.zeros((self.out_features,)))\n",
    "        return input @ weight.T + bias\n",
    "\n",
    "\n",
    "input = jnp.ones([10, 1])\n",
    "\n",
    "lazy = LazyLinear(out_features=1)\n",
    "\n",
    "print(f\"Layer before param is set:\\t{lazy}\")\n",
    "\n",
    "# `value_and_tree` executes any mutating method in a functional way\n",
    "_, material = sk.value_and_tree(lambda layer: layer(input))(lazy)\n",
    "\n",
    "print(f\"Layer after param is set:\\t{material}\")\n",
    "# subsequent calls will not set the parameters again\n",
    "material(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] Intermediates handling.\n",
    "\n",
    "This example shows how to capture specific intermediate values within each function call in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intermediate values:\t\n",
      " (Array([[0. ],\n",
      "       [0.5],\n",
      "       [1. ],\n",
      "       [1.5],\n",
      "       [2. ]], dtype=float32), Array([[-0.09999937],\n",
      "       [ 0.40000063],\n",
      "       [ 0.90000063],\n",
      "       [ 1.4000006 ],\n",
      "       [ 1.9000006 ]], dtype=float32))\n",
      "\n",
      "Final tree:\t\n",
      " Tree(a=0.801189)\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import serket as sk\n",
    "import jax\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@sk.autoinit\n",
    "class Tree(sk.TreeClass):\n",
    "    a: float = 1.0\n",
    "\n",
    "    def __call__(self, x: jax.Array, intermediate: tuple[Any, ...]):\n",
    "        x = x + self.a\n",
    "        # store intermediate variables\n",
    "        return x, intermediate + (x,)\n",
    "\n",
    "\n",
    "def loss_func(tree: Tree, x: jax.Array, y: jax.Array, intermediate: tuple[Any, ...]):\n",
    "    ypred, intermediate = tree(x, intermediate)\n",
    "    loss = jnp.mean((ypred - y) ** 2)\n",
    "    return loss, intermediate\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(\n",
    "    tree: Tree,\n",
    "    optim_state: optax.OptState,\n",
    "    x: jax.Array,\n",
    "    y: jax.Array,\n",
    "    intermediate: tuple[Any, ...],\n",
    "):\n",
    "    grads, intermediate = jax.grad(loss_func, has_aux=True)(tree, x, y, intermediate)\n",
    "    updates, optim_state = optim.update(grads, optim_state)\n",
    "    tree = optax.apply_updates(tree, updates)\n",
    "    return tree, optim_state, intermediate\n",
    "\n",
    "\n",
    "tree = Tree()\n",
    "optim = optax.adam(1e-1)\n",
    "optim_state = optim.init(tree)\n",
    "\n",
    "x = jnp.linspace(-1, 1, 5)[:, None]\n",
    "y = x**2\n",
    "\n",
    "intermediate = ()\n",
    "\n",
    "for i in range(2):\n",
    "    tree, optim_state, intermediate = train_step(tree, optim_state, x, y, intermediate)\n",
    "\n",
    "\n",
    "print(\"Intermediate values:\\t\\n\", intermediate)\n",
    "print(\"\\nFinal tree:\\t\\n\", tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Data pipelines\n",
    "\n",
    "In this example, `at` is used in similar fashion to [PyFunctional](https://github.com/EntilZha/PyFunctional) to work on general data pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "\n",
    "\n",
    "class Transaction:\n",
    "    def __init__(self, reason, amount):\n",
    "        self.reason = reason\n",
    "        self.amount = amount\n",
    "\n",
    "\n",
    "# this example copied from  https://github.com/EntilZha/PyFunctional\n",
    "transactions = [\n",
    "    Transaction(\"github\", 7),\n",
    "    Transaction(\"food\", 10),\n",
    "    Transaction(\"coffee\", 5),\n",
    "    Transaction(\"digitalocean\", 5),\n",
    "    Transaction(\"food\", 5),\n",
    "    Transaction(\"riotgames\", 25),\n",
    "    Transaction(\"food\", 10),\n",
    "    Transaction(\"amazon\", 200),\n",
    "    Transaction(\"paycheck\", -1000),\n",
    "]\n",
    "\n",
    "where = jax.tree_map(lambda x: x.reason == \"food\", transactions)\n",
    "food_cost = sk.at(transactions)[where].reduce(lambda x, y: x + y.amount, initializer=0)\n",
    "food_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] Regularization\n",
    "\n",
    "The following code showcase how to use `at` functionality to select some leaves of a model based on boolean mask or/and name condition to apply some weight regualrization on them. For example using `.at[...]` functionality the following can be achieved concisely:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean-based mask\n",
    "\n",
    "The entries of the arrays or leaves are selected based on a tree of the same structure but with boolean (`True`/`False`) leave. The `True` leaf points to place where the operation can be done, while `False` leaf is indicating that this leaf should not be touched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8333335\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "\n",
    "class Net(sk.TreeClass):\n",
    "    def __init__(self):\n",
    "        self.weight = jnp.array([-1, -2, -3, 1, 2, 3])\n",
    "        self.bias = jnp.array([-1, 1])\n",
    "\n",
    "\n",
    "def negative_entries_l2_loss(net: Net):\n",
    "    return (\n",
    "        # select all positive array entries\n",
    "        net.at[jax.tree_map(lambda x: x > 0, net)]\n",
    "        # set them to zero to exclude their loss\n",
    "        .set(0)\n",
    "        # select all leaves\n",
    "        .at[...]\n",
    "        # finally reduce with l2 loss\n",
    "        .reduce(lambda x, y: x + jnp.mean(y**2), initializer=0)\n",
    "    )\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(negative_entries_l2_loss(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name-based mask\n",
    "\n",
    "In this step, the mask is based on the path of the leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(\n",
      "  in_features=(1), \n",
      "  out_features=1, \n",
      "  in_axis=(-1), \n",
      "  out_axis=-1, \n",
      "  weight_init=glorot_uniform, \n",
      "  bias_init=zeros, \n",
      "  weight=f32[1,1](Î¼=0.20, Ïƒ=0.00, âˆˆ[0.20,0.20]), \n",
      "  bias=f32[1](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# note that `weight` is a leaf node in this layer\n",
    "# the `weight` leaf will be selected later in the next example.\n",
    "print(repr(sk.nn.Linear(1, 1, key=jax.random.PRNGKey(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.83809\n"
     ]
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Net(sk.TreeClass):\n",
    "    def __init__(self, key: jax.Array) -> None:\n",
    "        k1, k2, k3, k4 = jax.random.split(key, 4)\n",
    "        self.linear1 = sk.nn.Linear(in_features=1, out_features=20, key=k1)\n",
    "        self.linear2 = sk.nn.Linear(in_features=20, out_features=20, key=k2)\n",
    "        self.linear3 = sk.nn.Linear(in_features=20, out_features=20, key=k3)\n",
    "        self.linear4 = sk.nn.Linear(in_features=20, out_features=1, key=k4)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = jax.nn.tanh(self.linear1(x))\n",
    "        x = jax.nn.tanh(self.linear2(x))\n",
    "        x = jax.nn.tanh(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def linear_12_weight_l1_loss(net: Net):\n",
    "    return (\n",
    "        # select desired branches (linear1, linear2 in this example)\n",
    "        # and the desired leaves (weight)\n",
    "        net.at[\"linear1\", \"linear2\"][\"weight\"]\n",
    "        # alternatively, regex can be used to do the same functiontality\n",
    "        # >>> import re\n",
    "        # >>> net.at[re.compile(\"linear[12]\")][\"weight\"]\n",
    "        # finally apply l1 loss\n",
    "        .reduce(lambda x, y: x + jnp.sum(jnp.abs(y)), initializer=0)\n",
    "    )\n",
    "\n",
    "\n",
    "net = Net(key=jax.random.PRNGKey(0))\n",
    "print(linear_12_weight_l1_loss(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe can then be included inside the loss function, for example\n",
    "\n",
    "``` python\n",
    "\n",
    "def loss_fnc(net, x, y):\n",
    "    l1_loss = linear_12_weight_l1_loss(net)\n",
    "    loss += l1_loss\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] Sharing/Tie Weights\n",
    "\n",
    "In this example a simple `AutoEncoder` with shared `weight` between the encode/decoder is demonstrated. \n",
    "\n",
    "The key idea here, is that  sharing weight takes effect only within methods and does not extend beyond that scope. The limited scope design is to comply with `jax` functional requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "\n",
    "class AutoEncoder(sk.TreeClass):\n",
    "    def __init__(self, *, key: jax.Array):\n",
    "        k1, k2, k3, k4 = jr.split(key, 4)\n",
    "        self.enc1 = sk.nn.Linear(1, 10, key=k1)\n",
    "        self.enc2 = sk.nn.Linear(10, 20, key=k2)\n",
    "        self.dec2 = sk.nn.Linear(20, 10, key=k3)\n",
    "        self.dec1 = sk.nn.Linear(10, 1, key=k4)\n",
    "\n",
    "    def _tied_call(self, input: jax.Array) -> jax.Array:\n",
    "        # share/tie weights of encoder and decoder\n",
    "        # however this operation mutates the state\n",
    "        # so this method will only work with `value_and_tree`\n",
    "        # otherwise will throw `AttributeError`\n",
    "        self.dec1.weight = self.enc1.weight.T\n",
    "        self.dec2.weight = self.enc2.weight.T\n",
    "        output = self.enc1(input)\n",
    "        output = self.enc2(output)\n",
    "        output = self.dec2(output)\n",
    "        output = self.dec1(output)\n",
    "        return output\n",
    "\n",
    "    def tied_call(self, input: jax.Array) -> jax.Array:\n",
    "        # this method call `_tied_call` with value_and_tree\n",
    "        # return the output without mutating the state of the network\n",
    "        output, _ = sk.value_and_tree(lambda net: net._tied_call(input))(self)\n",
    "        return output\n",
    "\n",
    "    def non_tied_call(self, x):\n",
    "        output = self.enc1(x)\n",
    "        output = self.enc2(output)\n",
    "        output = self.dec2(output)\n",
    "        output = self.dec1(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "tree = sk.tree_mask(AutoEncoder(key=jr.PRNGKey(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(\n",
      "  in_features=(#10), \n",
      "  out_features=#1, \n",
      "  in_axis=(#-1), \n",
      "  out_axis=#-1, \n",
      "  weight_init=#glorot_uniform, \n",
      "  bias_init=#zeros, \n",
      "  weight=f32[1,10](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00]), \n",
      "  bias=f32[1](Î¼=0.35, Ïƒ=0.00, âˆˆ[0.35,0.35])\n",
      ") Linear(\n",
      "  in_features=(#20), \n",
      "  out_features=#10, \n",
      "  in_axis=(#-1), \n",
      "  out_axis=#-1, \n",
      "  weight_init=#glorot_uniform, \n",
      "  bias_init=#zeros, \n",
      "  weight=f32[10,20](Î¼=0.00, Ïƒ=0.00, âˆˆ[0.00,0.00]), \n",
      "  bias=f32[10](Î¼=0.11, Ïƒ=0.09, âˆˆ[-0.03,0.24])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "@jax.grad\n",
    "def tied_loss_func(net, x, y):\n",
    "    net = sk.tree_unmask(net)\n",
    "    return jnp.mean((jax.vmap(net.tied_call)(x) - y) ** 2)\n",
    "\n",
    "\n",
    "tree = sk.tree_mask(tree)\n",
    "x = jnp.ones([10, 1]) + 0.0\n",
    "y = jnp.ones([10, 1]) * 2.0\n",
    "grads: AutoEncoder = tied_loss_func(tree, x, y)\n",
    "# note that the shared weights have 0 gradient\n",
    "print(repr(grads.dec1), repr(grads.dec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(\n",
      "  in_features=(#10), \n",
      "  out_features=#1, \n",
      "  in_axis=(#-1), \n",
      "  out_axis=#-1, \n",
      "  weight_init=#glorot_uniform, \n",
      "  bias_init=#zeros, \n",
      "  weight=f32[1,10](Î¼=-0.12, Ïƒ=1.01, âˆˆ[-1.18,1.63]), \n",
      "  bias=f32[1](Î¼=-2.74, Ïƒ=0.00, âˆˆ[-2.74,-2.74])\n",
      ") Linear(\n",
      "  in_features=(#20), \n",
      "  out_features=#10, \n",
      "  in_axis=(#-1), \n",
      "  out_axis=#-1, \n",
      "  weight_init=#glorot_uniform, \n",
      "  bias_init=#zeros, \n",
      "  weight=f32[10,20](Î¼=-0.00, Ïƒ=0.35, âˆˆ[-1.59,1.02]), \n",
      "  bias=f32[10](Î¼=-0.88, Ïƒ=0.57, âˆˆ[-1.65,0.07])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check for non-tied call\n",
    "@jax.jit\n",
    "@jax.grad\n",
    "def tied_loss_func(net, x, y):\n",
    "    net = sk.tree_unmask(net)\n",
    "    return jnp.mean((jax.vmap(net.non_tied_call)(x) - y) ** 2)\n",
    "\n",
    "\n",
    "tree = sk.tree_mask(tree)\n",
    "x = jnp.ones([10, 1]) + 0.0\n",
    "y = jnp.ones([10, 1]) * 2.0\n",
    "grads: AutoEncoder = tied_loss_func(tree, x, y)\n",
    "\n",
    "# note that the shared weights have non-zero gradients\n",
    "print(repr(grads.dec1), repr(grads.dec2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [6] Extract intermediate gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': Array(8., dtype=float32, weak_type=True),\n",
       " 'c': Array(4., dtype=float32, weak_type=True)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import serket as sk\n",
    "import jax\n",
    "\n",
    "\n",
    "class Foo(sk.TreeClass):\n",
    "    def __init__(self):\n",
    "        self.a = 1.0\n",
    "\n",
    "    def __call__(self, x, perturb):\n",
    "        b = self.a + x + perturb[\"b\"]\n",
    "        c = 2 * b + perturb[\"c\"]\n",
    "        e = 4 * c\n",
    "        return e\n",
    "\n",
    "\n",
    "foo = Foo()\n",
    "\n",
    "# de/dc = 4\n",
    "# de/db = de/dc * dc/db = 4 * 2 = 8\n",
    "\n",
    "# take gradient with respect to the perturbations pytree\n",
    "# by setting `argnums=1` in `jax.grad`\n",
    "inter_grads = jax.grad(foo, argnums=1)(1.0, dict(b=0.0, c=0.0))\n",
    "inter_grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
